![CI](https://github.com/MSMalak/ResearchPaperQA/actions/workflows/tests.yml/badge.svg)

# ğŸ“„ ResearchPaperQA

> RAG pipeline for querying research papers from the terminal.

**Ask questions to research papers using a local RAG pipeline (PDF â†’ FAISS â†’ LLM).**

ResearchPaperQA is a lightweight Retrieval-Augmented Generation (RAG) demo that allows you to query research papers (PDFs) directly from the terminal using semantic search and language models.

---

## ğŸš€ Features

* ğŸ“š PDF ingestion and chunking
* ğŸ” Semantic retrieval with Sentence-Transformers + FAISS
* ğŸ¤– Answer generation with:

  * Local HuggingFace models (default)
  * OpenAI models (optional)
* ğŸ§© Modular architecture (loader / embedder / vector store / generator)
* ğŸ–¥ï¸ Clean CLI interface (`researchpaperqa`)
* ğŸ” Reproducible indexing with cache control

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/MSMalak/ResearchPaperQA.git
cd ResearchPaperQA

python -m venv venv
source venv/bin/activate
pip install -e .
```

> The project is installed in *editable mode* so changes are picked up automatically.

---

## â–¶ï¸ Quickstart

Build the vector index and start querying papers:

```bash
researchpaperqa --documents data/sample_papers --recreate
```

Then ask questions directly in the terminal, for example:

* *â€œWhat is the main contribution of this paper?â€*
* *â€œWhich methodology is used?â€*
* *â€œWhat problem does the paper address?â€*

---

## ğŸ–¥ï¸ UI Demo (optional)

An optional Streamlit interface is provided for quick interactive demos.

```bash
pip install -r requirements-ui.txt
streamlit run app.py
```

> Note: Local generation can be slower depending on hardware.
For faster responses, prefer the CLI with the OpenAI backend (`--generator openai`) if an API key is available.
---

## ğŸ§  How it works

1. Load PDFs from a directory
2. Split documents into semantic chunks
3. Embed chunks using Sentence-Transformers
4. Index embeddings with FAISS
5. Retrieve top-k relevant chunks for a query
6. Generate an answer conditioned on retrieved context

```
PDFs â†’ Chunking â†’ Embeddings â†’ FAISS â†’ Retrieval â†’ Answer
```

---

## âš™ï¸ CLI Usage

```bash
researchpaperqa --help
```

```text
usage: researchpaperqa [-h] [--documents DOCUMENTS]
                       [--generator {local,openai}] [--recreate]

ResearchPaperQA â€” RAG chatbot over research PDFs
```

### Options

* `--documents` : path to a directory containing PDF files
* `--generator` : `local` (HuggingFace) or `openai`
* `--recreate`  : force rebuilding the vector index

---

## ğŸ” Notes on security & reproducibility

* The FAISS index uses pickle-based serialization internally.
* Deserialization is enabled only for locally created indexes.
* Vector indexes and metadata are intentionally excluded from version control.

---

## ğŸ§ª Project structure

```text
rag_chatbot/
â”œâ”€â”€ loader.py        # PDF loading & chunking
â”œâ”€â”€ embedder.py      # Embedding models
â”œâ”€â”€ vectorstore.py   # FAISS index management
â”œâ”€â”€ generator.py    # LLM backends
â”œâ”€â”€ main.py          # CLI entry point
```

---

## ğŸ“Œ Limitations & future work

* UI provided as an optional Streamlit demo (CLI remains the primary interface)
* Single-document indexing (for now)
* Potential extensions:

  * Multi-document comparison
  * Streaming answers
  * Web or notebook interface
  * Evaluation of retrieval quality

---

## ğŸ“„ License

MIT License



